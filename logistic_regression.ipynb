{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      0       0       0       0       1       2       0       0       0   \n",
      "1      1       0       0       0       0       0       0       0       0   \n",
      "2      0       0       0       0       0       1       0       0       0   \n",
      "3      1       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0       0  ...         3         0         0         0         0         1   \n",
      "1       0  ...       203       214       166         0         0         0   \n",
      "2       0  ...       164       177       163         0         0         1   \n",
      "3       0  ...         9        10         9         9         8         1   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         1         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "0       0       0       0       0       0       0       0       9       8   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0        0  ...       103        87        56         0         0         0   \n",
      "1        0  ...         0         0         0         0         0         0   \n",
      "2        0  ...        57        70        28         0         2         0   \n",
      "3        0  ...         0         0         0         0         0         0   \n",
      "4        3  ...        39        31        19        19         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"fashion_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"fashion_test.csv\")\n",
    "\n",
    "# Display dataset structure\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (9600, 784)\n",
      "Validation set shape: (2400, 784)\n",
      "Test set shape: (2000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Extract features (X) and labels (y)\n",
    "X = train_data.drop(columns=[\"label\"]).values / 255.0  # Normalize\n",
    "y = train_data[\"label\"].values  # Target labels\n",
    "\n",
    "# Split training set into Training (80%) & Validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize test set\n",
    "X_test = test_data.values / 255.0\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, input_dim, learning_rate=0.01):\n",
    "        self.W = np.random.randn(input_dim) * 0.01  # Small random weights\n",
    "        self.b = 0  # Bias initialized to 0\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Compute probability predictions.\"\"\"\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Classify into 0 or 1 based on probability threshold 0.5.\"\"\"\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Binary Cross-Entropy Loss.\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000):\n",
    "        \"\"\"Train model using Stochastic Gradient Descent (SGD).\"\"\"\n",
    "        m = X_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            y_pred_train = self.predict_proba(X_train)\n",
    "            y_pred_val = self.predict_proba(X_val)\n",
    "\n",
    "            train_loss = self.compute_loss(y_train, y_pred_train)\n",
    "            val_loss = self.compute_loss(y_val, y_pred_val)\n",
    "\n",
    "            # Compute gradients\n",
    "            dW = np.dot(X_train.T, (y_pred_train - y_train)) / m\n",
    "            db = np.mean(y_pred_train - y_train)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "\n",
    "            # Print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Validation Loss = {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.7030, Validation Loss = 0.7029\n",
      "Epoch 100: Train Loss = 0.1648, Validation Loss = 0.1629\n",
      "Epoch 200: Train Loss = 0.1283, Validation Loss = 0.1258\n",
      "Epoch 300: Train Loss = 0.1142, Validation Loss = 0.1113\n",
      "Epoch 400: Train Loss = 0.1065, Validation Loss = 0.1034\n",
      "Epoch 500: Train Loss = 0.1016, Validation Loss = 0.0984\n",
      "Epoch 600: Train Loss = 0.0982, Validation Loss = 0.0948\n",
      "Epoch 700: Train Loss = 0.0956, Validation Loss = 0.0922\n",
      "Epoch 800: Train Loss = 0.0936, Validation Loss = 0.0901\n",
      "Epoch 900: Train Loss = 0.0920, Validation Loss = 0.0885\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train model\n",
    "model = LogisticRegression(input_dim=X_train.shape[1], learning_rate=0.01)\n",
    "model.train(X_train, y_train, X_val, y_val, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9742\n",
      "Precision: 0.9719\n",
      "Recall: 0.9768\n",
      "F1-score: 0.9743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "precision = precision_score(y_val, y_pred_val)\n",
    "recall = recall_score(y_val, y_pred_val)\n",
    "f1 = f1_score(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle submission file saved as 'submission_baseline.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_test_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Save submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": np.arange(len(y_test_pred_proba)),\n",
    "    \"prediction\": y_test_pred_proba  # Use raw probability values\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"submission_baseline.csv\", index=False)\n",
    "print(\"Kaggle submission file saved as 'submission_baseline.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
