{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework4 - CSC 480\n",
    "#### Name: Sreyas Gangji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      0       0       0       0       1       2       0       0       0   \n",
      "1      1       0       0       0       0       0       0       0       0   \n",
      "2      0       0       0       0       0       1       0       0       0   \n",
      "3      1       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0       0  ...         3         0         0         0         0         1   \n",
      "1       0  ...       203       214       166         0         0         0   \n",
      "2       0  ...       164       177       163         0         0         1   \n",
      "3       0  ...         9        10         9         9         8         1   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         1         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "0       0       0       0       0       0       0       0       9       8   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0        0  ...       103        87        56         0         0         0   \n",
      "1        0  ...         0         0         0         0         0         0   \n",
      "2        0  ...        57        70        28         0         2         0   \n",
      "3        0  ...         0         0         0         0         0         0   \n",
      "4        3  ...        39        31        19        19         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"fashion_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"fashion_test.csv\")\n",
    "\n",
    "# Display dataset structure\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (9600, 784)\n",
      "Validation set shape: (2400, 784)\n",
      "Test set shape: (2000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Extract features (X) and labels (y)\n",
    "X = train_data.drop(columns=[\"label\"]).values / 255.0  # Normalize\n",
    "y = train_data[\"label\"].values  # Target labels\n",
    "\n",
    "# Split training set into Training (80%) & Validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize test set\n",
    "X_test = test_data.values / 255.0\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, input_dim, learning_rate=0.01):\n",
    "        self.W = np.random.randn(input_dim) * 0.01  # Small random weights\n",
    "        self.b = 0  # Bias initialized to 0\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Compute probability predictions.\"\"\"\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Classify into 0 or 1 based on probability threshold 0.5.\"\"\"\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Binary Cross-Entropy Loss.\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000):\n",
    "        \"\"\"Train model using Stochastic Gradient Descent (SGD).\"\"\"\n",
    "        m = X_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            y_pred_train = self.predict_proba(X_train)\n",
    "            y_pred_val = self.predict_proba(X_val)\n",
    "\n",
    "            train_loss = self.compute_loss(y_train, y_pred_train)\n",
    "            val_loss = self.compute_loss(y_val, y_pred_val)\n",
    "\n",
    "            # Compute gradients\n",
    "            dW = np.dot(X_train.T, (y_pred_train - y_train)) / m\n",
    "            db = np.mean(y_pred_train - y_train)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "\n",
    "            # Print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Validation Loss = {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.7030, Validation Loss = 0.7029\n",
      "Epoch 100: Train Loss = 0.1648, Validation Loss = 0.1629\n",
      "Epoch 200: Train Loss = 0.1283, Validation Loss = 0.1258\n",
      "Epoch 300: Train Loss = 0.1142, Validation Loss = 0.1113\n",
      "Epoch 400: Train Loss = 0.1065, Validation Loss = 0.1034\n",
      "Epoch 500: Train Loss = 0.1016, Validation Loss = 0.0984\n",
      "Epoch 600: Train Loss = 0.0982, Validation Loss = 0.0948\n",
      "Epoch 700: Train Loss = 0.0956, Validation Loss = 0.0922\n",
      "Epoch 800: Train Loss = 0.0936, Validation Loss = 0.0901\n",
      "Epoch 900: Train Loss = 0.0920, Validation Loss = 0.0885\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train model\n",
    "model = LogisticRegression(input_dim=X_train.shape[1], learning_rate=0.01)\n",
    "model.train(X_train, y_train, X_val, y_val, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9742\n",
      "Precision: 0.9719\n",
      "Recall: 0.9768\n",
      "F1-score: 0.9743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "precision = precision_score(y_val, y_pred_val)\n",
    "recall = recall_score(y_val, y_pred_val)\n",
    "f1 = f1_score(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle submission file saved as 'submission_baseline.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_test_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Save submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": np.arange(len(y_test_pred_proba)),\n",
    "    \" class\": y_test_pred_proba  # Use raw probability values\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"submission_baseline.csv\", index=False)\n",
    "print(\"Kaggle submission file saved as 'submission_baseline.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with more epoch (2000 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.6917, Validation Loss = 0.6910\n",
      "Epoch 100: Train Loss = 0.1638, Validation Loss = 0.1616\n",
      "Epoch 200: Train Loss = 0.1278, Validation Loss = 0.1249\n",
      "Epoch 300: Train Loss = 0.1139, Validation Loss = 0.1106\n",
      "Epoch 400: Train Loss = 0.1063, Validation Loss = 0.1029\n",
      "Epoch 500: Train Loss = 0.1014, Validation Loss = 0.0979\n",
      "Epoch 600: Train Loss = 0.0980, Validation Loss = 0.0944\n",
      "Epoch 700: Train Loss = 0.0955, Validation Loss = 0.0918\n",
      "Epoch 800: Train Loss = 0.0935, Validation Loss = 0.0898\n",
      "Epoch 900: Train Loss = 0.0919, Validation Loss = 0.0882\n",
      "Epoch 1000: Train Loss = 0.0906, Validation Loss = 0.0868\n",
      "Epoch 1100: Train Loss = 0.0894, Validation Loss = 0.0857\n",
      "Epoch 1200: Train Loss = 0.0884, Validation Loss = 0.0847\n",
      "Epoch 1300: Train Loss = 0.0875, Validation Loss = 0.0838\n",
      "Epoch 1400: Train Loss = 0.0867, Validation Loss = 0.0830\n",
      "Epoch 1500: Train Loss = 0.0860, Validation Loss = 0.0823\n",
      "Epoch 1600: Train Loss = 0.0854, Validation Loss = 0.0817\n",
      "Epoch 1700: Train Loss = 0.0847, Validation Loss = 0.0811\n",
      "Epoch 1800: Train Loss = 0.0842, Validation Loss = 0.0806\n",
      "Epoch 1900: Train Loss = 0.0836, Validation Loss = 0.0801\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression model with 2000 epochs\n",
    "model_epochs2000 = LogisticRegression(input_dim=X_train.shape[1], learning_rate=0.01)\n",
    "model_epochs2000.train(X_train, y_train, X_val, y_val, epochs=2000)  # Increased epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Kaggle submission file saved as 'submission_epochs2000.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_test_pred_epochs2000 = model_epochs2000.predict_proba(X_test)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_epochs2000 = pd.DataFrame({\n",
    "    \"id\": np.arange(len(y_test_pred_epochs2000)),  # Ensure IDs start from 0\n",
    "    \" class\": y_test_pred_epochs2000  # Ensure correct column name\n",
    "})\n",
    "\n",
    "# Save the CSV file\n",
    "submission_epochs2000.to_csv(\"submission_epochs2000.csv\", index=False, encoding=\"utf-8\", float_format=\"%.6f\")\n",
    "\n",
    "print(\"New Kaggle submission file saved as 'submission_epochs2000.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with a lower training rate (0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.6937, Validation Loss = 0.6936\n",
      "Epoch 100: Train Loss = 0.4556, Validation Loss = 0.4550\n",
      "Epoch 200: Train Loss = 0.3468, Validation Loss = 0.3459\n",
      "Epoch 300: Train Loss = 0.2875, Validation Loss = 0.2863\n",
      "Epoch 400: Train Loss = 0.2505, Validation Loss = 0.2491\n",
      "Epoch 500: Train Loss = 0.2252, Validation Loss = 0.2237\n",
      "Epoch 600: Train Loss = 0.2069, Validation Loss = 0.2052\n",
      "Epoch 700: Train Loss = 0.1929, Validation Loss = 0.1911\n",
      "Epoch 800: Train Loss = 0.1819, Validation Loss = 0.1799\n",
      "Epoch 900: Train Loss = 0.1730, Validation Loss = 0.1709\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression model with a lower learning rate (0.001)\n",
    "model_lr0001 = LogisticRegression(input_dim=X_train.shape[1], learning_rate=0.001)\n",
    "model_lr0001.train(X_train, y_train, X_val, y_val, epochs=1000)  # Standard 1000 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Kaggle submission file saved as 'submission_lr0001.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_test_pred_lr0001 = model_lr0001.predict_proba(X_test)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_lr0001 = pd.DataFrame({\n",
    "    \"id\": np.arange(len(y_test_pred_lr0001)),  # Ensure IDs start from 0\n",
    "    \" class\": y_test_pred_lr0001  # Ensure correct column name\n",
    "})\n",
    "\n",
    "# Save the CSV file\n",
    "submission_lr0001.to_csv(\"submission_lr0001.csv\", index=False, encoding=\"utf-8\", float_format=\"%.6f\")\n",
    "\n",
    "print(\"New Kaggle submission file saved as 'submission_lr0001.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the learning rate too much caused the model to learn too slowly, leading to poorer performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with Mini-Batch Gradient Descent (Batch Size = 64)\n",
    "\n",
    "1. Learning Rate = 0.01 (since 0.001 was too slow)\n",
    "2. Batch Size = 64 (instead of full-batch training)\n",
    "3. Epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionMiniBatch:\n",
    "    def __init__(self, input_dim, learning_rate=0.01, batch_size=64):\n",
    "        self.W = np.random.randn(input_dim) * 0.01\n",
    "        self.b = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000):\n",
    "        m = X_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(m)  # Shuffle data\n",
    "            X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "            for i in range(0, m, self.batch_size):\n",
    "                X_batch = X_train[i:i+self.batch_size]\n",
    "                y_batch = y_train[i:i+self.batch_size]\n",
    "\n",
    "                y_pred_batch = self.sigmoid(np.dot(X_batch, self.W) + self.b)\n",
    "\n",
    "                dW = np.dot(X_batch.T, (y_pred_batch - y_batch)) / self.batch_size\n",
    "                db = np.mean(y_pred_batch - y_batch)\n",
    "\n",
    "                self.W -= self.learning_rate * dW\n",
    "                self.b -= self.learning_rate * db\n",
    "\n",
    "# Train the Mini-Batch SGD Model\n",
    "model_minibatch = LogisticRegressionMiniBatch(input_dim=X_train.shape[1], learning_rate=0.01, batch_size=64)\n",
    "model_minibatch.train(X_train, y_train, X_val, y_val, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Kaggle submission file saved as 'submission_minibatch.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_test_pred_minibatch = model_minibatch.sigmoid(np.dot(X_test, model_minibatch.W) + model_minibatch.b)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_minibatch = pd.DataFrame({\n",
    "    \"id\": np.arange(len(y_test_pred_minibatch)),  # Ensure IDs start from 0\n",
    "    \" class\": y_test_pred_minibatch  # Ensure correct column name\n",
    "})\n",
    "\n",
    "# Save the CSV file\n",
    "submission_minibatch.to_csv(\"submission_minibatch.csv\", index=False, encoding=\"utf-8\", float_format=\"%.6f\")\n",
    "\n",
    "print(\"New Kaggle submission file saved as 'submission_minibatch.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with L2 Regularization (λ = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionL2:\n",
    "    def __init__(self, input_dim, learning_rate=0.01, reg_lambda=0.1):\n",
    "        self.W = np.random.randn(input_dim) * 0.01\n",
    "        self.b = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda  # L2 Regularization term\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000):\n",
    "        m = X_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.sigmoid(np.dot(X_train, self.W) + self.b)\n",
    "            \n",
    "            # Compute gradients with L2 Regularization\n",
    "            dW = np.dot(X_train.T, (y_pred - y_train)) / m + self.reg_lambda * self.W\n",
    "            db = np.mean(y_pred - y_train)\n",
    "\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "\n",
    "# Train the L2 Regularized Model\n",
    "model_l2 = LogisticRegressionL2(input_dim=X_train.shape[1], learning_rate=0.01, reg_lambda=0.1)\n",
    "model_l2.train(X_train, y_train, X_val, y_val, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Kaggle submission file saved as 'submission_l2.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_test_pred_l2 = model_l2.sigmoid(np.dot(X_test, model_l2.W) + model_l2.b)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_l2 = pd.DataFrame({\n",
    "    \"id\": np.arange(len(y_test_pred_l2)),  # Ensure IDs start from 0\n",
    "    \" class\": y_test_pred_l2  # Ensure correct column name\n",
    "})\n",
    "\n",
    "# Save the CSV file\n",
    "submission_l2.to_csv(\"submission_l2.csv\", index=False, encoding=\"utf-8\", float_format=\"%.6f\")\n",
    "\n",
    "print(\"New Kaggle submission file saved as 'submission_l2.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with Mini-Batch Gradient Descent (Batch Size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch Gradient Descent with Batch Size = 32\n",
    "model_minibatch_32 = LogisticRegressionMiniBatch(input_dim=X_train.shape[1], learning_rate=0.01, batch_size=32)\n",
    "model_minibatch_32.train(X_train, y_train, X_val, y_val, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Kaggle submission file saved as 'submission_minibatch_32.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_test_pred_minibatch_32 = model_minibatch_32.sigmoid(np.dot(X_test, model_minibatch_32.W) + model_minibatch_32.b)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_minibatch_32 = pd.DataFrame({\n",
    "    \"id\": np.arange(len(y_test_pred_minibatch_32)),  # Ensure IDs start from 0\n",
    "    \" class\": y_test_pred_minibatch_32  # Ensure correct column name\n",
    "})\n",
    "\n",
    "# Save the CSV file\n",
    "submission_minibatch_32.to_csv(\"submission_minibatch_32.csv\", index=False, encoding=\"utf-8\", float_format=\"%.6f\")\n",
    "\n",
    "print(\"New Kaggle submission file saved as 'submission_minibatch_32.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with Learning Rate Decay\n",
    "\n",
    "1. Initial LR: 0.01\n",
    "2. Decay Rate: 0.99 (Each epoch, LR = LR * 0.99)\n",
    "3. Epochs: 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionDecay:\n",
    "    def __init__(self, input_dim, initial_lr=0.01, decay_rate=0.99):\n",
    "        self.W = np.random.randn(input_dim) * 0.01\n",
    "        self.b = 0\n",
    "        self.initial_lr = initial_lr\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000):\n",
    "        m = X_train.shape[0]\n",
    "        learning_rate = self.initial_lr  # Start with initial LR\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.sigmoid(np.dot(X_train, self.W) + self.b)\n",
    "\n",
    "            dW = np.dot(X_train.T, (y_pred - y_train)) / m\n",
    "            db = np.mean(y_pred - y_train)\n",
    "\n",
    "            # Apply learning rate decay\n",
    "            learning_rate *= self.decay_rate  # Reduce LR in each epoch\n",
    "\n",
    "            self.W -= learning_rate * dW\n",
    "            self.b -= learning_rate * db\n",
    "\n",
    "# Train model with Learning Rate Decay\n",
    "model_decay = LogisticRegressionDecay(input_dim=X_train.shape[1], initial_lr=0.01, decay_rate=0.99)\n",
    "model_decay.train(X_train, y_train, X_val, y_val, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Kaggle submission file saved as 'submission_decay.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_test_pred_decay = model_decay.sigmoid(np.dot(X_test, model_decay.W) + model_decay.b)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_decay = pd.DataFrame({\n",
    "    \"id\": np.arange(len(y_test_pred_decay)),  # Ensure IDs start from 0\n",
    "    \" class\": y_test_pred_decay  # Ensure correct column name\n",
    "})\n",
    "\n",
    "# Save the CSV file\n",
    "submission_decay.to_csv(\"submission_decay.csv\", index=False, encoding=\"utf-8\", float_format=\"%.6f\")\n",
    "\n",
    "print(\"New Kaggle submission file saved as 'submission_decay.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
